{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import cv2\n",
    "import random \n",
    "import pickle \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1500 whithout accidents images\n",
      "There are 1357 whith accidents images\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "without_accd_dir = 'C:/ACCIDENT_DATASET_v1/No Accident'\n",
    "with_accd_dir = 'C:/ACCIDENT_DATASET_v1/Accident'\n",
    "imgs_without_acc = [file for file in os.listdir(without_accd_dir) if file.endswith('.jpg')]\n",
    "print('There are',len(imgs_without_acc), 'whithout accidents images')\n",
    "imgs_with_acc = [file for file in os.listdir(with_accd_dir) if file.endswith('.jpg')]\n",
    "print('There are',len(imgs_with_acc), 'whith accidents images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dir = 'C:/ACCIDENT_DATASET_v1/AccDetectSplited/'\n",
    "os.mkdir(new_dir)\n",
    "train_folder = os.path.join(new_dir, 'train')\n",
    "train_accident = os.path.join(train_folder, 'accident')\n",
    "train_noaccident= os.path.join(train_folder, 'no_accident')\n",
    "\n",
    "val_folder = os.path.join(new_dir, 'validation')\n",
    "val_accident = os.path.join(val_folder, 'accident')\n",
    "val_noaccident = os.path.join(val_folder, 'no_accident')\n",
    "\n",
    "os.mkdir(train_folder)\n",
    "os.mkdir(train_accident)\n",
    "os.mkdir(train_noaccident)\n",
    "\n",
    "os.mkdir(val_folder)\n",
    "os.mkdir(val_accident)\n",
    "os.mkdir(val_noaccident)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train no_accident\n",
    "import shutil\n",
    "imgs = imgs_without_acc[:1000]\n",
    "for img in imgs:\n",
    "    origin = os.path.join(without_accd_dir, img)\n",
    "    destination = os.path.join(train_noaccident, img)\n",
    "    shutil.copyfile(origin, destination)\n",
    "    \n",
    "# validation no_accident\n",
    "imgs = imgs_without_acc[1000:]\n",
    "for img in imgs:\n",
    "    origin = os.path.join(without_accd_dir, img)\n",
    "    destination = os.path.join(val_noaccident, img)\n",
    "    shutil.copyfile(origin, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train accident\n",
    "imgs = imgs_with_acc[:1000]\n",
    "for img in imgs:\n",
    "    origin = os.path.join(with_accd_dir, img)\n",
    "    destination = os.path.join(train_accident, img)\n",
    "    shutil.copyfile(origin, destination)\n",
    "    \n",
    "# validation accident\n",
    "imgs = imgs_with_acc[1000:]\n",
    "for img in imgs:\n",
    "    origin = os.path.join(with_accd_dir, img)\n",
    "    destination = os.path.join(val_accident, img)\n",
    "    shutil.copyfile(origin, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(CATEGORIES,DATADIR,IMG_SIZE):## chargement du dataset(ensembles des images 'with-accident' et 'without-accident')\n",
    "    data = []\n",
    "    for category in CATEGORIES:  \n",
    "        path = os.path.join(DATADIR,category)\n",
    "        class_num = CATEGORIES.index(category)  \n",
    "        for img in os.listdir(path):   \n",
    "            try:\n",
    "                img_array = cv2.imread(os.path.join(path,img))   \n",
    "                img_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))  \n",
    "                data.append([img_array, class_num])  \n",
    "            except Exception as e:  \n",
    "                pass\n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(création des données d'entrainement et du test)\n",
    "IMG_SIZE = 300 \n",
    "cat = ['accident','no_accident']\n",
    "training_path = 'C:/ACCIDENT_DATASET_v1/AccDetectSplited/train'\n",
    "testing_path = 'C:/ACCIDENT_DATASET_v1/AccDetectSplited/validation'\n",
    "\n",
    "training_data = create_data(cat, training_path, IMG_SIZE)\n",
    "testing_data = create_data(cat, testing_path, IMG_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_data : 2000\n",
      "testing_data : 857\n"
     ]
    }
   ],
   "source": [
    "print(\"training_data :\",len(training_data))\n",
    "print(\"testing_data :\",len(testing_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#séparation des images et leurs labels\n",
    "def lebalization(data):\n",
    "    random.shuffle(data) \n",
    "    X = []\n",
    "    y = []\n",
    "    for features,label in data:\n",
    "        X.append(features)\n",
    "        y.append(label)\n",
    "    return  np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = lebalization(training_data)\n",
    "X_test, y_test = lebalization(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalization des images \n",
    "X_train =X_train.reshape(-1, IMG_SIZE, IMG_SIZE,3)\n",
    "X_test = X_test.reshape(-1, IMG_SIZE, IMG_SIZE, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 300, 300, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = X_train[:700] / 255 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2 = X_train[700:1400] / 255 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train3 = X_train[1400:] / 255 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train0=np.concatenate((X_train1,X_train2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.concatenate((X_train0,X_train3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test / 255 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(857, 300, 300, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arret de l'entrainement en cas de l'accuracy >0.99\n",
    "class myCallbacks(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self,epoch,logs={}):\n",
    "        if(logs.get('accuracy')>0.991):\n",
    "            print (\"\\nReached 99% of accuracy\")\n",
    "            self.model.stop_training=True       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    # Note the input shape is the desired size of the image 300x300 with 1 bytes color\n",
    "    # This is the first convolution\n",
    "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=X_train.shape[1:]),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    \n",
    "    # The second convolution\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2), \n",
    "    \n",
    "    # The third convolution\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2), \n",
    "    \n",
    "    # The fourth convolution  \n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2), \n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    \n",
    "    # Flatten the results to feed into a DNN\n",
    "    tf.keras.layers.Flatten(),\n",
    "    # 512 neuron hidden layer\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('horses') and 1 for the other ('humans')\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "]) \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.compile(loss='binary_crossentropy',\n",
    "#              optimizer=RMSprop( learning_rate=1e-4),\n",
    "#              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 298, 298, 16)      448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 149, 149, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 147, 147, 32)      4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 73, 73, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 71, 71, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 35, 35, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 33, 33, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               2097280   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,157,921\n",
      "Trainable params: 2,157,921\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "63/63 [==============================] - 141s 2s/step - loss: 0.6517 - accuracy: 0.6115\n",
      "Epoch 2/50\n",
      "63/63 [==============================] - 113s 2s/step - loss: 0.5292 - accuracy: 0.7240\n",
      "Epoch 3/50\n",
      "63/63 [==============================] - 111s 2s/step - loss: 0.4865 - accuracy: 0.7545\n",
      "Epoch 4/50\n",
      "63/63 [==============================] - 112s 2s/step - loss: 0.4313 - accuracy: 0.8010\n",
      "Epoch 5/50\n",
      "63/63 [==============================] - 114s 2s/step - loss: 0.3929 - accuracy: 0.8225\n",
      "Epoch 6/50\n",
      "63/63 [==============================] - 113s 2s/step - loss: 0.3492 - accuracy: 0.8505\n",
      "Epoch 7/50\n",
      "63/63 [==============================] - 113s 2s/step - loss: 0.2870 - accuracy: 0.8775\n",
      "Epoch 8/50\n",
      "63/63 [==============================] - 112s 2s/step - loss: 0.2298 - accuracy: 0.9050\n",
      "Epoch 9/50\n",
      "63/63 [==============================] - 109s 2s/step - loss: 0.2054 - accuracy: 0.9180\n",
      "Epoch 10/50\n",
      "63/63 [==============================] - 109s 2s/step - loss: 0.1458 - accuracy: 0.9425\n",
      "Epoch 11/50\n",
      "63/63 [==============================] - 108s 2s/step - loss: 0.1310 - accuracy: 0.9510\n",
      "Epoch 12/50\n",
      "63/63 [==============================] - 109s 2s/step - loss: 0.0924 - accuracy: 0.9680\n",
      "Epoch 13/50\n",
      "63/63 [==============================] - 109s 2s/step - loss: 0.0787 - accuracy: 0.9700\n",
      "Epoch 14/50\n",
      "63/63 [==============================] - 110s 2s/step - loss: 0.0807 - accuracy: 0.9705\n",
      "Epoch 15/50\n",
      "63/63 [==============================] - 108s 2s/step - loss: 0.0471 - accuracy: 0.9825\n",
      "Epoch 16/50\n",
      "63/63 [==============================] - 108s 2s/step - loss: 0.0377 - accuracy: 0.9870\n",
      "Epoch 17/50\n",
      "63/63 [==============================] - 108s 2s/step - loss: 0.0581 - accuracy: 0.9830\n",
      "Epoch 18/50\n",
      "63/63 [==============================] - 108s 2s/step - loss: 0.0295 - accuracy: 0.9905\n",
      "Epoch 19/50\n",
      "63/63 [==============================] - 108s 2s/step - loss: 0.0287 - accuracy: 0.9915\n",
      "\n",
      "Reached 99% of accuracy\n"
     ]
    }
   ],
   "source": [
    "earlyStop = myCallbacks()\n",
    "history = model.fit(X_train, y_train, epochs=50,callbacks=[earlyStop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation du modèle\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "probs = model.predict(X_test)\n",
    "#preds = probs[:,1]\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_test, probs)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "# method I: plt\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10))\n",
    "#ax1.plot(history.history['loss'], color='b', label=\"Training loss\")\n",
    "#ax1.plot(history.history['val_loss'], color='r', label=\"validation loss\")\n",
    "#ax1.set_xticks(np.arange(1, 21, 1))\n",
    "#ax1.set_yticks(np.arange(0, 1, 0.1))\n",
    "\n",
    "ax2.plot(history.history['accuracy'], color='b', label=\"training accuracy\")\n",
    "ax2.plot(history.history['loss'], color='r',label=\"training loss\")\n",
    "ax2.set_xticks(np.arange(1, 22, 1))\n",
    "\n",
    "legend = plt.legend(loc='best', shadow=True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction en utilisant le dataset du test\n",
    "y_predicted = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_test[6].reshape(300,300,3)) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted_flattened=y_predicted.reshape(-1,)\n",
    "y_predicted_flattened[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted_labels =[]\n",
    "for i in y_predicted_flattened:\n",
    "    if i>0.5:\n",
    "        y_predicted_labels.append(1)\n",
    "    else:\n",
    "        y_predicted_labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=0)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] \n",
    "    else:\n",
    "        1 \n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Function to calculate accuracy \n",
    "def cal_accuracy(model,X_test,y_test): \n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = np.round(y_pred)\n",
    "    print (\"Accuracy : \", round(accuracy_score(y_test,y_pred)*100,2))\n",
    "    print (\"Precision : \", round(precision_score(y_test,y_pred)*100,2))\n",
    "    print (\"Sensitivity / Recall : \", round(recall_score(y_test,y_pred)*100,2))\n",
    "    print (\"F1_score : \", round(f1_score(y_test,y_pred)*100,2)) \n",
    "    cnf_matrix_tra = confusion_matrix(y_test, y_pred)\n",
    "    class_names = [0,1]\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cnf_matrix_tra , classes=class_names, title='Confusion matrix')\n",
    "    plt.show()\n",
    "    print(\"Report : \") \n",
    "    print(classification_report(digits=6,y_true=y_test, y_pred=y_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cal_accuracy(model,X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=input('Enter URL of Image :')\n",
    "\n",
    "#img=imread(url)\n",
    "img=cv2.imread(url,3)\n",
    "plt.imshow(img) \n",
    "img_resize = cv2.resize(img, (300, 300))  \n",
    "new_img = img_resize.reshape(-1, IMG_SIZE, IMG_SIZE, 3) \n",
    "\n",
    "y_predicted = model.predict(new_img)\n",
    "print(y_predicted[0]) \n",
    "print(\"car\",cat[int(y_predicted[0])]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Model_CNN_ACCIDENT_DATASET_v3_93%.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
